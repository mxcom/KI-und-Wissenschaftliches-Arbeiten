\section{Schachkommentator}

Jetzt werden ich den Teil besprechen, der für die Erzeugung von Kommentaren verantwortlich ist.

\subsection{Encoder-Decoder Modell}

\begin{itemize}[leftmargin=*]
\item Erzeugung von Kommentaren fällt in Bereich Sequenz-zu-Sequenz-Verarbeitung
\item Input (Entsprechende Informationen) wird auf Output (d.h. menschliche Sprachen) dargestellt
\item Länge von Input und Output können sich dabei unterscheiden
\item Architektur die dies ermöglicht ist das Encoder-Decoder Modell
\item Basiert auf speziellen neuronalen Netzwerkarchitektur, bidirektionalen Long-Short Term Memory-Architektur (LSTM)
\end{itemize}

\newpage

\begin{itemize}[leftmargin=*]
\item LSTMs sind Erweiterung von Rekurrenten Neuronalen Netzwerken
\item RNNs neuronale Netze, zur Verarbeitung von Sequenzen
\begin{itemize}[leftmargin=*]
\item Generierte Outputs wieder mit neuem Input in Netzwerk geben
\item Zustand des Netzes repräsentiert Neuronen zu bestimmten Zeitpunkt
\item Durch zurückführen des Outputs kann sich Netzwerk an laufende Muster erinnern und entsprechend reagieren
\end{itemize}
\item Bidirektionale RNNs sind Erweiterung von RNNs
\item Eingaben nicht nur in positive Zeitrichtung, sondern auch in negative Zeitrichtung berücksichtigt
\item Sowohl vorherigen als auch nachfolgenden Eingabedaten einbezogen
\item Problem von RNNs: Bei langen Sequenzen hat interne Gedächtnis Probleme laufende Muster zu merken
\item Problem nennt man "Problem des verschwindenden Gradienten"
\item Lösung sind spezielle Neuronen, den Long-Short Term-Memory Neuronen
\item Verfügen über spezielles Kurzzeitgedächtnis und Langzeitgedächtnis
\end{itemize}

Bidirektionale LSTMs sind also bidirektionale RNNs, die Long-Short Term Memory Neuronen verwenden.

\newpage

\begin{itemize}[leftmargin=*]
\item Encoder-Decoder Modell besteht aus zwei Teilen: Encoder, Decoder
\item Encoder und Decoder sind bidirektionale LSTMs
\item Encoder empfängt als Input Informationen von Schach-Engine 
\item Wandelt diese in andere Darstellung um
\item Darstellung ist eine Zusammenfassung der Informationen
\item Zusammenfassung mit Hilfe von Aufmerksamkeitsmechanismus erstellt
\item Decoder verwendet Darstellung und erstellt entsprechende Kommentare
\end{itemize}

\newpage

\begin{itemize}[leftmargin=*]
\item Abbildung eines Encoder-Decoder Modells mitgebracht
\item Blöcke stellen Netzwerk zu bestimmten Zeitpunkt dar
\item Entsprechende Sequenz wird von links nach rechts, zeitlich aufsteigend, eingegeben
\item Hidden Layers verarbeiten die Sequenz
\item Fassen durch Aufmerksamkeitsmechanismus bestimmte Teile der Eingabe in Endzustand zusammen
\item Zustand wird als Kontextvektor bezeichnet
\item Kontextvektor wird zur Initialisierung der Neuronen des Decoders verwendet
\item Generiung beginnt mit Startzeichen
\item Ausgabe stellt erzeugten Kommentar dar
\item Ausgabe wird als neue Eingabe in den Decoder gegeben
\item Passiert solange bis Endzeichen erreicht wird
\end{itemize}

\newpage

\subsection{Generationsmodelle}

\begin{itemize}[leftmargin=*]
\item Um zu wissen zu was genau man Kommentare generieren soll, wurde 5 Kategorien festgelegt.\\
\item Einziges Encoder-Decoder Modell reicht nicht aus
\item Für jede Kategorien existiert ein Encoder-Decoder Modell
\item Unterscheiden sich im Training
\item Kategorien werden deshalb als Generationsmodelle bezeichnet
\item Encoder die mehrere Züge bekommen als Multi-Move-Encoder bezeichnet
\item Encoder die nur einen Zug bekommen als Single-Move-Encoder bezeichnet
\item Merkmale die die Modelle erhalten, lassen sich als Funktion zusammenfassen
\item Zug mit $m$ (move) abgekürzt und Position mit $b$ (board).
\end{itemize}

Nun da man weiß was für Merkmale man zum trainieren brauch, wurden Datensätze zum Trainieren, erstellt, welche basierend auf Kommentaren aus den fünf Kategorien aus einem Schachforum stammen.